---
title: "ML R1 prediction models for serotype O"
format: html
editor: visual
---

## Apply ML models to predict cross-protection using R1 values

Recomended R1 cutoff vlaues for crosspretction is 0.3, where it is assumed that \>0.3 vlaues indicated cross protection between two viruses (serum vs. virus pairs). However, the accuracy of this cutoff has been debated but it is still the prescribed and used approach.

Variations of two ML approaches will be explored with minor tweaks to achive the best models

## Visualize phylogenetic tree

See distribution of sequences on a phylogenetic tree to assess representation of serum/vaccines and viruses used in this analysis

```{r}
pacman::p_load(ggtree,ggplot2,scales,stringr,muscle,lubridate,bioseq,dplyr,zoo,tidytree,
               readxl,janitor,Biostrings,qualpalr,phytools,treeio,tidyr,phylotools,
               picante,ape,vegan,tidyverse,Hmisc)
source("G:\\My Drive\\Local D\\R&Stata toolbox\\theme_dennis.R")
path <- "G:/My Drive/Desktop/To_Do/FMD in EA-HORN/"
# plot tree with anotations----
tree.O <- read.tree(paste0(path,paste("Synthesis files/R1/sero.o/ML R1 serotype O/RAxML_bestTree.tree")))
met <- read.csv(paste0(path,paste("Data files/R1 data/serotype.O.met.csv"))) %>% 
  glimpse()
met$accession <- gsub(" ", "", met$accession, fixed = TRUE) #works better than trimws

pal= qualpal(9, colorspace=list(h=c(0,360), s=c(0.3,1), l=c(0.2,0.8)))

tr <- ggtree(tree.O,right = F, lwd =0.8,aes(color=as.factor(serum.vaccine))) %<+% met + 
  theme_tree()+
  #geom_tiplab(aes(label = id),align = T, size=2)+
  geom_tippoint(aes(color=as.factor(topotype)), size = 2.5)+
  #geom_line(aes(color=topotype))+
  scale_color_manual(name= "Serum/vaccine",values = pal$hex);tr 

```

## ML model development

See distribution of sequences on a phylogenetic tree to assess representation of serum/vaccines and viruses used in this analysis

```{r}
# Set up analytical pipeline based on PRRSV models----
pacman::p_load(shiny,shinydashboard,shinythemes,dashboardthemes,shinyWidgets,
               shinyStore,rpart,caTools,caret,rpart.plot,randomForest,ROSE,DMwR2,
               randomForestExplainer,pROC,ROCR,missForest,Rforestry,gbm,pdp,ggplot2, iml, 
               Boruta,plyr,dplyr,ggpubr,janitor,readxl,bioseq,muscle,ape,
               BiocManager,BiocVersion,phylotools,seqinr,reshape2,DECIPHER)
#load data
#input_data <-   read_fasta(file.choose())
input_data <-   read_fasta(paste0(path,paste("Data files/R1 data/serotype.O.aligned.fasta"))) %>% glimpse()
input_data0 = c(input_data)
input_data1 <-  seq_translate(input_data0)
input_data2 <- muscle::muscle(AAStringSet(input_data1),quiet = F)
input_data3 <- msa::msaConvert(input_data2,type="seqinr::alignment")
mat2 <- (dist.alignment(input_data3, matrix = "identity",gap = F))^2#=p-distance in MEGA10

#convert distance matrix into data frame
df2 <- melt(as.matrix(mat2), varnames = c("row", "col"))
df2 <- df2[df2$row != df2$col,]
df2$row <- gsub("\r","",df2$row)
df2$col <- gsub("\r","",df2$col)
df2 <- df2 %>% 
  #rename(aa.dist=value) %>%
  mutate(aa.dist=value,
         id=as.factor(paste(row,"|",col,sep = ""))) %>% 
  select(id, everything())%>%
  subset ( select = -c(row,col,value))

#suppress warnings here
oldw <- getOption("warn") 
options(warn = -1)
# use data files to generate numerical positions matrix
name = names(input_data0)
sequence = paste(seq_translate(input_data0))
input_data.gbdf.o <- data.frame(name,(stringr::str_split_fixed(sequence, "", max(nchar(sequence)))))

# code position differences
input_data.dat.gbdf.o <- data.frame()

for(i in 1:nrow(input_data.gbdf.o)){
  #for first row of data, check it against all other rows
  row.i <- input_data.gbdf.o[i,]
  row.no.i <- subset(input_data.gbdf.o,name != input_data.gbdf.o$name[i])
  out.i <- data.frame(name=rep(input_data.gbdf.o$name[i],nrow(row.no.i)),name2=row.no.i$name)
  # j <- 2
  # z <- 1
  for(j in 2:ncol(row.no.i)){
    input_data <- ifelse(row.i[,j] == row.no.i[,j],1,0)#gives vector for that column on matching
    
    out.i <- cbind(out.i,input_data)
    names(out.i)[3:ncol(out.i)] <- paste("p",seq(1:ncol(out.i)),sep="")
  }
  input_data.dat.gbdf.o <- rbind(input_data.dat.gbdf.o,out.i)
  
}

input_data.dat.gbdf.o$name  <- mgsub::mgsub(input_data.dat.gbdf.o$name, 
                         c("c",",","'", " ","[(]","\\s",'"'), c("","","","","","",""))
input_data.dat.gbdf.o$name2  <- mgsub::mgsub(input_data.dat.gbdf.o$name2, 
                         c("c",",","'", " ","[(]","\\s",'"'), c("","","","","","",""))
input_data.dat.gbdf.o$id <- paste(input_data.dat.gbdf.o$name,"|",input_data.dat.gbdf.o$name2,sep = "")
input_data.dat.gbdf.o <- input_data.dat.gbdf.o %>% select(id, everything())%>%
  subset ( select = -c(name,name2)) %>% glimpse()
#cols <- colnames(input_data.dat.gbdf.o[-1])
input_data.dat.gbdf.o[] <- lapply(input_data.dat.gbdf.o[], factor) 

# get r1 values
r1 <- read.csv(paste0(path,paste("Data files/R1 data/serotype.O.csv"))) %>% 
  clean_names()%>%
  glimpse()
r1 <- subset(r1, select = c(accession_v1,accession_s1,r1)) 
r1 <- r1[complete.cases(r1), ] %>%  glimpse()
r1$accession_v1  <- mgsub::mgsub(r1$accession_v1, 
                                            c("c",",","'", " ","[(]","\\s",'"'), c("","","","","","",""))
r1$accession_s1  <- mgsub::mgsub(r1$accession_s1, 
                                             c("c",",","'", " ","[(]","\\s",'"'), c("","","","","","",""))
r1$id <- paste(r1$accession_v1,"|",r1$accession_s1,sep = "")
r1 <- r1 %>% select(id, everything())%>%
  subset ( select = -c(accession_v1,accession_s1)) %>% glimpse()

on.exit(options(warn = oldw))#allow warnings back

# combine r1, aa dist and positions
df_final <- left_join(r1, df2, by="id") %>% glimpse()
df_final <- left_join(df_final, input_data.dat.gbdf.o, by="id") %>% 
  .[complete.cases(df_final$aa.dist),] %>% glimpse()
write.csv(df_final,file = "Data files/R1 data/r1_aa_plus.csv",row.names = F)


```

```{r}
cor.test(df_final$r1,df_final$aa.dist,method = "spearman")#-0.36 p= 0.0001125
o.cor.plt <- ggscatter(df_final,
                                   x="r1", 
                                   y="aa.dist", 
                                   merge = T, add = "reg.line",
                                   conf.int = F, cor.coef = T, cor.method = "spearman",
                                   xlab ="P amino acid distance" ,ylab ="R1 values" ,
                                   title = "")+
  theme_dennis(); o.cor.plt
ggsave("Synthesis files/R1/sero.o/o.cor.plt.pdf", plot = o.cor.plt, width = 16,
       height = 10, dpi = 400, units = "in");dev.off()


```

\# model set up\-\-\--

```{r}
# 1) random forest approach
df_final <- read.csv(paste0(path,paste("Data files/R1 data/r1_aa_plus.csv"))) %>% glimpse()
df_final[-c(1:3)] <- lapply(df_final[-c(1:3)], factor) 
df_final <- df_final %>%
  mutate(r1.1=as.factor(ifelse(r1<0.3,0,1))) %>%
  relocate(r1.1,.after = r1) %>%
  subset(., select=-c(id,r1)) %>% 
  glimpse()
# store 20% of data for model validation later
set.seed(1567)
split <- sample.split(df_final$r1.1, SplitRatio = 0.8)
df_final.mod <- df_final[split==T,] %>% glimpse()#87 pairs
df_final.valid <- df_final[split==F,]%>% glimpse()#21 pairs

# split remaining data into training and test data
split <- sample.split(df_final.mod$r1.1, SplitRatio = 0.8)
df_final.train <- df_final.mod[split==T,]%>% glimpse()#70
df_final.test <- df_final.mod[split==F,]%>% glimpse()#17

# design and tune model with all columns
set.seed(1567)
features <- setdiff(names(df_final.train),"r1.1")

hyper_grid <- expand.grid(
  mtry       = seq(2, ncol(df_final.train) * 0.8, by = 2),
  nodesize  = seq(2, 20, by = 2),
  samplesize = c(.55, .632, .70, .80),
  OOB   = 0,
  bal.acc =0
  
)
nrow(hyper_grid)
set.seed(1567)
for (i in 1:nrow(hyper_grid)) {
  # Train a Random Forest model
  model <- randomForest(r1.1~., data=df_final.train, 
                        ntree = 500,
                        mtry = hyper_grid$mtry[i],importance=F,
                        nodesize = hyper_grid$nodesize[i],
                        sample.fraction = hyper_grid$samplesize[i])
  
  # Store OOB error for the model                      
  hyper_grid$OOB[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
  p <- predict(model,newdata=df_final.test) #Predictions on Test Set for each Tree
  ba <- confusionMatrix(p,df_final.test$r1.1,positive="1")$byClass[11]#record the balanced accuracy
  hyper_grid$bal.acc[i] <- ba 
}
hyper.dat <- hyper_grid %>% 
  dplyr::arrange(desc(bal.acc)) %>%
  #head(10) %>% # see top ten combinations
  dplyr::slice(1);hyper.dat # get best model has 

opt.sero.o.rf <- randomForest(r1.1~., data=df_final.train, 
                                  type= "classification",
                                  ntree = 500,
                                  mtry = hyper.dat$mtry,
                                  nodesize = hyper.dat$nodesize,
                                  sample.fraction = hyper.dat$samplesize,
                                  importance=T)


opt.sero.o.rf #14.29% 

p <- predict(opt.sero.o.rf,newdata=df_final.test) #Predictions on Test Set for each Tree
confusionMatrix(p,df_final.test$r1.1,positive="1") # Bal acc 0.8824
roc.curve(df_final.test$r1.1,p)#NA


```

\# model accuracy is good but the sensitivity is horrible, try improving it

```{r}

# A) Add cross validation
control <- trainControl(method="adaptive_cv", number=10, repeats=100)
opt.sero.o.rf.cv <- randomForest(r1.1~., data=df_final.train, 
                       method='rf',
                       type= "classification",
                       ntree = 500,
                       mtry = hyper.dat$mtry,
                       nodesize = hyper.dat$nodesize,
                       sample.fraction = hyper.dat$samplesize,
                       trControl=control,
                       metric='Accuracy',
                       importance=T,
                       seed=1567)



opt.sero.o.rf.cv #15.71% 

p <- predict(opt.sero.o.rf.cv,newdata=df_final.test) #Predictions on Test Set for each Tree
confusionMatrix(p,df_final.test$r1.1,positive="1") # Bal acc 0.9412
roc.curve(df_final.test$r1.1,p)#NA 
  # this model is great! check performance on novel data
p <- predict(opt.sero.o.rf.cv,newdata=df_final.valid) 
confusionMatrix(p,df_final.valid$r1.1,positive="1") # Bal acc 0.9048
roc.curve(df_final.valid$r1.1,p)#NA

# B)Remove sites with no variation----
## manually
df_final <- read.csv(paste0(path,paste("Data files/R1 data/r1_aa_plus.csv"))) %>% glimpse()
df_final.sum <-as.data.frame( df_final[,-c(1:3)] %>%
                    #replace(is.na(.), 0) %>%
                    summarise_all(sum))

df_final.sum <- as.data.frame(t(df_final.sum)) %>% glimpse()
df_final.sum$site <- colnames(df_final[,-c(1:3)])
df_final.sum <- df_final.sum %>% relocate (site)
df_final.sum$V1 <- as.numeric(df_final.sum$V1)
df_final.sum$keep <- ifelse(214-df_final.sum$V1 >=107,1,0) #0.2*214
df_final.keep <- c("id","r1","aa.dist",df_final.sum[df_final.sum$keep==1,]$site)

df_final.trim <- df_final[,df_final.keep] %>% glimpse() # smaller dataset

df_final.trim[-c(1:3)] <- lapply(df_final.trim[-c(1:3)], factor) 
df_final.trim <- df_final.trim %>%
  mutate(r1.1=as.factor(ifelse(r1<0.3,0,1))) %>%
  relocate(r1.1,.after = r1) %>%
  subset(., select=-c(id,r1)) %>% 
  glimpse()
# store 20% of data for model validation later
set.seed(1567)
split <- sample.split(df_final.trim$r1.1, SplitRatio = 0.8)
df_final.trim.mod <- df_final.trim[split==T,] %>% glimpse()#87 pairs
df_final.trim.valid <- df_final.trim[split==F,]%>% glimpse()#21 pairs

# split remaining data into training and test data
split <- sample.split(df_final.trim.mod$r1.1, SplitRatio = 0.8)
df_final.trim.train <- df_final.trim.mod[split==T,]%>% glimpse()#70
df_final.trim.test <- df_final.trim.mod[split==F,]%>% glimpse()#17

# design and tune model with trimmed data and run model with cv
set.seed(1567)
features <- setdiff(names(df_final.trim.train),"r1.1")

hyper_grid <- expand.grid(
  mtry       = seq(2, ncol(df_final.trim.train) * 0.8, by = 2),
  nodesize  = seq(2, 20, by = 2),
  samplesize = c(.55, .632, .70, .80),
  OOB   = 0,
  bal.acc =0
  
)
nrow(hyper_grid)
set.seed(1567)
for (i in 1:nrow(hyper_grid)) {
  # Train a Random Forest model
  model <- randomForest(r1.1~., data=df_final.trim.train, 
                        ntree = 500,
                        mtry = hyper_grid$mtry[i],importance=F,
                        nodesize = hyper_grid$nodesize[i],
                        sample.fraction = hyper_grid$samplesize[i])
  
  # Store OOB error for the model                      
  hyper_grid$OOB[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
  p <- predict(model,newdata=df_final.trim.test) #Predictions on Test Set for each Tree
  ba <- confusionMatrix(p,df_final.trim.test$r1.1,positive="1")$byClass[11]#record the balanced accuracy
  hyper_grid$bal.acc[i] <- ba 
}
hyper.dat <- hyper_grid %>% 
  dplyr::arrange(desc(bal.acc)) %>%
  #head(10) %>% # see top ten combinations
  slice(1);hyper.dat # get best model has 

# use best model form above (cv+tuning)
control <- trainControl(method="adaptive_cv", number=10, repeats=100)
opt.sero.o.rf.trim.cv <- randomForest(r1.1~., data=df_final.trim.train, 
                                 method='rf',
                                 type= "classification",
                                 ntree = 500,
                                 mtry = hyper.dat$mtry,
                                 nodesize = hyper.dat$nodesize,
                                 sample.fraction = hyper.dat$samplesize,
                                 trControl=control,
                                 metric='Accuracy',
                                 importance=T,
                                 seed=1567)



opt.sero.o.rf.trim.cv #14.29% 

p <- predict(opt.sero.o.rf.trim.cv,newdata=df_final.trim.test) #Predictions on Test Set for each Tree
confusionMatrix(p,df_final.trim.test$r1.1,positive="1") # Bal acc 0.8824 
roc.curve(df_final.trim.test$r1.1,p)#NA 

# this model is no better from the earlier versions great 


# C)SMOTE to remedy imbalance and rerun cv model----
#devtools::install_version("DMwR",version="0.4.1")
library(DMwR)

smote.df_final.train <- SMOTE(r1.1~., data = df_final.train, perc.over = 200, perc.under = 200)
table(smote.df_final.train$r1.1)

control <- trainControl(method="adaptive_cv", number=10, repeats=100)
opt.sero.o.rf.cv.smote <- randomForest(r1.1~., data=smote.df_final.train, 
                                 method='rf',
                                 type= "classification",
                                 ntree = 500,
                                 mtry = hyper.dat$mtry,
                                 nodesize = hyper.dat$nodesize,
                                 sample.fraction = hyper.dat$samplesize,
                                 trControl=control,
                                 metric='Accuracy',
                                 importance=T,
                                 seed=1567)



opt.sero.o.rf.cv.smote #6.12% 

p <- predict(opt.sero.o.rf.cv.smote,newdata=df_final.test) #Predictions on Test Set for each Tree
confusionMatrix(p,df_final.test$r1.1,positive="1") # Bal acc 0.9286
roc.curve(df_final.test$r1.1,p)#NA 
# this model is great! check performance on novel data
p <- predict(opt.sero.o.rf.cv,newdata=df_final.valid) 
confusionMatrix(p,df_final.valid$r1.1,positive="1") # Bal acc 0.8456
roc.curve(df_final.valid$r1.1,p)#NA
# this is great, keep model!

# D) Can combining BORUTA,SMOTE and CV improve the model more?
# add BORUTA sifting to the ectodomain----
y.o <- df_final[,2] %>% glimpse()
x.o <- df_final[,-c(1:2)] %>% glimpse()
set.seed(1567)
ImpVar.o <- Boruta(x.o, y.o, doTrace = 0, maxRuns = 100,mcAdj = T,
                       getImp = getImpRfZ)
print(ImpVar.o)
ret.df_final <- getSelectedAttributes(ImpVar.o, withTentative = T) %>%glimpse()
df_final.br <- subset(df_final, select=c("r1", ret.df_final)) %>% na.omit() %>% glimpse()

# data splits
df_final.br[-c(1:2)] <- lapply(df_final.br[-c(1:2)], factor) 
df_final.br <- df_final.br %>%
  mutate(r1.1=as.factor(ifelse(r1<0.3,0,1))) %>%
  relocate(r1.1,.after = r1) %>%
  subset(., select=-c(r1)) %>% 
  glimpse()
# store 20% of data for model validation later
set.seed(1567)
split <- sample.split(df_final.br$r1.1, SplitRatio = 0.8)
df_final.br.mod <- df_final.br[split==T,] %>% glimpse()#87 pairs
df_final.br.valid <- df_final.br[split==F,]%>% glimpse()#21 pairs

# split remaining data into training and test data
split <- sample.split(df_final.br.mod$r1.1, SplitRatio = 0.8)
df_final.br.train <- df_final.br.mod[split==T,]%>% glimpse()#70
df_final.br.test <- df_final.br.mod[split==F,]%>% glimpse()#17

# design and tune model with boruta selcted features
set.seed(1567)
features <- setdiff(names(df_final.br.train),"r1.1")

hyper_grid <- expand.grid(
  mtry       = seq(2, ncol(df_final.br.train) * 0.8, by = 2),
  nodesize  = seq(2, 20, by = 2),
  samplesize = c(.55, .632, .70, .80),
  OOB   = 0,
  bal.acc =0
  
)
nrow(hyper_grid)
set.seed(1567)
for (i in 1:nrow(hyper_grid)) {
  # Train a Random Forest model
  model <- randomForest(r1.1~., data=df_final.br.train, 
                        ntree = 500,
                        mtry = hyper_grid$mtry[i],importance=F,
                        nodesize = hyper_grid$nodesize[i],
                        sample.fraction = hyper_grid$samplesize[i])
  
  # Store OOB error for the model                      
  hyper_grid$OOB[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
  p <- predict(model,newdata=df_final.br.test) #Predictions on Test Set for each Tree
  ba <- confusionMatrix(p,df_final.br.test$r1.1,positive="1")$byClass[11]#record the balanced accuracy
  hyper_grid$bal.acc[i] <- ba 
}
hyper.dat <- hyper_grid %>% 
  dplyr::arrange(desc(bal.acc)) %>%
  #head(10) %>% # see top ten combinations
  slice(1);hyper.dat # get best model has 

opt.sero.o.br.rf <- randomForest(r1.1~., data=df_final.br.train, 
                              type= "classification",
                              ntree = 500,
                              mtry = hyper.dat$mtry,
                              nodesize = hyper.dat$nodesize,
                              sample.fraction = hyper.dat$samplesize,
                              importance=T)


opt.sero.o.br.rf #11.43% 


# apply SMOTE+cv model 
smote.df_final.br.train <- SMOTE(r1.1~., data = df_final.br.train, perc.over = 200, perc.under = 200)
table(smote.df_final.br.train$r1.1)

control <- trainControl(method="adaptive_cv", number=10, repeats=100)
opt.sero.o.br.rf.cv.smote <- randomForest(r1.1~., data=smote.df_final.br.train, 
                                       method='rf',
                                       type= "classification",
                                       ntree = 500,
                                       mtry = hyper.dat$mtry,
                                       nodesize = hyper.dat$nodesize,
                                       sample.fraction = hyper.dat$samplesize,
                                       trControl=control,
                                       metric='Accuracy',
                                       importance=T,
                                       seed=1567)



opt.sero.o.rf.cv.smote #6.12% 

p <- predict(opt.sero.o.br.rf.cv.smote,newdata=df_final.br.test) #Predictions on Test Set for each Tree
confusionMatrix(p,df_final.br.test$r1.1,positive="1") # Bal acc 0.9286
roc.curve(df_final.br.test$r1.1,p)#NA 
# this model is the same as the smote+cv on full all columns
p <- predict(opt.sero.o.br.rf.cv.smote,newdata=df_final.br.valid) 
confusionMatrix(p,df_final.br.valid$r1.1,positive="1") # Bal acc 0.8456
roc.curve(df_final.br.valid$r1.1,p)#NA

#E) How does xgboost compare to the best model?----
pacman::p_load(xgboost,Matrix,data.table,caTools)

# use data generated from the boruta+smote+cv model above
# option 1
smote.df_final.br.train.xgb <- smote.df_final.br.train %>% #option 1
  mutate(r1.1=as.character(r1.1)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  glimpse()
smote.df_final.br.train.xgb <- df_final.br.mod %>% #option 2 include rf testing data
  mutate(r1.1=as.character(r1.1)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  glimpse()  

df_final.br.test.xgb <- df_final.br.test %>%
  mutate(r1.1=as.character(r1.1)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  glimpse()
df_final.br.valid.xgb <- df_final.br.valid %>%
  mutate(r1.1=as.character(r1.1)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  glimpse()

labels <- as.numeric(smote.df_final.br.train.xgb$r1.1) 

dtrain <- xgb.DMatrix(label = labels, 
                      data = as.matrix(smote.df_final.br.train.xgb[,-1]))

labels.test <- as.numeric(df_final.br.test.xgb$r1.1) 

dtest <- xgb.DMatrix(label = labels.test, 
                      data = as.matrix(df_final.br.test.xgb[,-1]))

labels.valid <- as.numeric(df_final.br.valid.xgb$r1.1) 

dvalid <- xgb.DMatrix(label = labels.valid, 
                      data = as.matrix(df_final.br.valid.xgb[,-1]))


set.seed(1567)
# Model tuning
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.01, 
               gamma=0.1, max_depth=10, min_child_weight=1, subsample=1, 
               colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 1000, nfold = 10, 
                 showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, 
                 maximize = F)

eval.df.xgbcv <- as.data.frame(xgbcv[["evaluation_log"]]) %>% glimpse()
eval.df.xgbcv.train <- eval.df.xgbcv[,c(1:3)] %>% glimpse()
eval.df.xgbcv.train$id <- "train"
colnames(eval.df.xgbcv.train) <- c("iteration","logloss","std","id")
eval.df.xgbcv.test <- eval.df.xgbcv[,c(1,4,5)]
eval.df.xgbcv.test$id <- "test"
colnames(eval.df.xgbcv.test) <- c("iteration","logloss","std","id")
eval.df.xgbcv.long <- rbind(eval.df.xgbcv.train,eval.df.xgbcv.test)
# ggplot(eval.df.xgbcv.long,aes(x=iteration,y=logloss, color=id))+
#   geom_line(position = "identity",size=1)
# ggplot(eval.df.xgbcv.long,aes(x=iteration,y=std, color=id))+
#   geom_line(position = "identity",size=1)
which.min(eval.df.xgbcv$test_logloss_mean)#60 rounds

# Set nrounds 
set.seed(1567)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = (which.min(eval.df.xgbcv$test_logloss_mean)), 
                   print_every_n = 2, 
                   early_stop_round = 5, maximize = F , eval_metric = "error",
                   eval_metric="auc")

#model prediction
xgbpred <- predict (xgb1,dtrain)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred.df <- as.data.frame(xgbpred) %>% glimpse()
#confusion matrix
pacman::p_load(caret,pROC)
confusionMatrix (factor(xgbpred), factor(labels),positive = "1" )#Accuracy : 0.6863  
roc.curve(factor(xgbpred), smote.df_final.br.train.xgb$r1.1)
#model validation
xgbpred <- predict (xgb1,dvalid)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred.df <- as.data.frame(xgbpred) %>% glimpse()
#confusion matrix
confusionMatrix (factor(xgbpred), factor(labels.valid),positive = "1" )#Accuracy : 0.6863  
roc.curve(factor(xgbpred), df_final.br.valid.xgb$r1.1)

## option2
smote.df_final.br.train.xgb <- df_final.br.mod %>% #option 2 include rf testing data
  mutate(r1.1=as.character(r1.1)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  glimpse()  


labels <- as.numeric(smote.df_final.br.train.xgb$r1.1) 

dtrain <- xgb.DMatrix(label = labels, 
                      data = as.matrix(smote.df_final.br.train.xgb[,-1]))


set.seed(1567)
# Model tuning
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.01, 
               gamma=0.1, max_depth=10, min_child_weight=1, subsample=1, 
               colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 1000, nfold = 10, 
                 showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, 
                 maximize = F)

eval.df.xgbcv <- as.data.frame(xgbcv[["evaluation_log"]]) %>% glimpse()
eval.df.xgbcv.train <- eval.df.xgbcv[,c(1:3)] %>% glimpse()
eval.df.xgbcv.train$id <- "train"
colnames(eval.df.xgbcv.train) <- c("iteration","logloss","std","id")
eval.df.xgbcv.test <- eval.df.xgbcv[,c(1,4,5)]
eval.df.xgbcv.test$id <- "test"
colnames(eval.df.xgbcv.test) <- c("iteration","logloss","std","id")
eval.df.xgbcv.long <- rbind(eval.df.xgbcv.train,eval.df.xgbcv.test)
# ggplot(eval.df.xgbcv.long,aes(x=iteration,y=logloss, color=id))+
#   geom_line(position = "identity",size=1)
# ggplot(eval.df.xgbcv.long,aes(x=iteration,y=std, color=id))+
#   geom_line(position = "identity",size=1)
which.min(eval.df.xgbcv$test_logloss_mean)

# Set nrounds 
set.seed(1567)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = (which.min(eval.df.xgbcv$test_logloss_mean)), 
                   print_every_n = 2, 
                   early_stop_round = 5, maximize = F , eval_metric = "error",
                   eval_metric="auc")

#model prediction
xgbpred <- predict (xgb1,dtrain)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred.df <- as.data.frame(xgbpred) %>% glimpse()
#confusion matrix
pacman::p_load(caret,pROC)
confusionMatrix (factor(xgbpred), factor(labels),positive = "1" )#Accuracy : 0.6863  
roc.curve(factor(xgbpred), smote.df_final.br.train.xgb$r1.1)
#model validation
xgbpred <- predict (xgb1,dvalid)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
xgbpred.df <- as.data.frame(xgbpred) %>% glimpse()
#confusion matrix
confusionMatrix (factor(xgbpred), factor(labels.valid),positive = "1" )#Accuracy : 0.6863  
roc.curve(factor(xgbpred), df_final.br.valid.xgb$r1.1)
## Although option 1 model performs considerably well, the predictions using novel
## data is not better than the best RF model. Notably though is the perfect 
## sensitivity of that xgb model. However, although when training with more data,
## option 2, model performance is still better than the best rf model, predictions
## on novel data are worse and the sensitivity is horrible (50%)


```

## Conclusion

At this point, the best publishable model is the boruta adjusted+ rf+smote+cv model (AUC=85%,Acc=90%\[69.6-98.8%\], Se=75%, Sp=94%, PPV=75%, NPV=94%).

Alternative ways to improve the model can include identifying epitopes and adding within epitope distances and site distances as features in the model.
